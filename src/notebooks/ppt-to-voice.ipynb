{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules and define folder paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# Llama Index imports for LMM \n",
    "from llama_index import (\n",
    "    ServiceContext,  set_global_service_context\n",
    ")\n",
    "from llama_index.llms import AzureOpenAI\n",
    "from llama_index.multi_modal_llms.azure_openai import AzureOpenAIMultiModal\n",
    "from llama_index.embeddings import AzureOpenAIEmbedding\n",
    "from llama_index.readers import SimpleDirectoryReader\n",
    "\n",
    "# Import Azure Cognitive Services speech SDK for TTS\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    # stream=sys.stdout, level=logging.DEBUG\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    "\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "\n",
    "root_folder_path = Path(\"C:/Users/61052067/data/pptread\")\n",
    "# input_folder_name = \"life_sciences_regulatory_101\"\n",
    "input_folder_name = \"life_sciences_regulatory_101_test\"\n",
    "# input_folder_name = \"life_sciences_regulatory_101_sample1\"\n",
    "process_folder_path = root_folder_path / input_folder_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a service Context with LLM, Multimodal Model and embedding model. Only Multimodal model is used in the implementation. \n",
    "LLM and embedding models are will be needed if future implementation requires and index to be created e.g. a summary index to created high level overview of the presentation content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_API_ENDPOINT\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "api_key_lmm = os.getenv(\"OPENAI_API_KEY_LMM\")\n",
    "azure_endpoint_lmm = os.getenv(\"AZURE_OPENAI_API_ENDPOINT_LMM\")\n",
    "api_version_lmm = os.getenv(\"AZURE_OPENAI_API_VERSION_LMM\")\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    engine=\"gpt-4\",\n",
    "    model=\"gpt-4\",\n",
    "    # engine=\"gpt-35-turbo-16k\",\n",
    "    # model=\"gpt-35-turbo-16k\",\n",
    "    temperature=0.0,\n",
    "    azure_endpoint= azure_endpoint,\n",
    "    api_key= api_key,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "multi_modal_llm = AzureOpenAIMultiModal(\n",
    "    engine=\"gpt-4\",\n",
    "    model=\"gpt-4-vision-preview\",\n",
    "    # engine=\"gpt-35-turbo-16k\",\n",
    "    # model=\"gpt-35-turbo-16k\",\n",
    "    temperature=0.0,\n",
    "    api_key= api_key_lmm,\n",
    "    azure_endpoint= azure_endpoint_lmm,\n",
    "    api_version=api_version_lmm,\n",
    "    max_tokens = 4096\n",
    ")\n",
    "\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"text-embedding-ada-002\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    "    chunk_size=1\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup and configure Azure Speech Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_key = os.getenv(\"AZURE_SPEECH_SERVICE_KEY\")\n",
    "service_region = os.getenv(\"AZURE_SPEECH_SERVICE_REGION\")\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "# speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config) # directly to speaker\n",
    "speech_config.speech_synthesis_voice_name = \"en-US-AvaNeural\"\n",
    "speech_config.set_speech_synthesis_output_format(speechsdk.SpeechSynthesisOutputFormat.Audio16Khz32KBitRateMonoMp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One shot example for content generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot_example = \"\"\"In the multifaceted ecosystem of life sciences, the organization is structured into several critical business entities, \n",
    "each with a distinct role in the journey from concept to market. The Research & Development arm spearheads this voyage with Drug Discovery, \n",
    "focusing on patent searches and candidate finalization, followed by Drug Development which encompasses analytical processes and packaging. \n",
    "Preclinical Studies and Clinical Trials form the bedrock of product validation. Parallel to these, Manufacturing & Supply Chain Operations \n",
    "ensure GMP compliance and seamless material handling, while Quality Control and Assurance uphold the product standards. In the commercial arena, \n",
    "Sales and Marketing Operations engage both digital and conventional marketing strategies to promote products and services, \n",
    "manage new product launches, and handle adverse events and complaints. Underpinning these specialized domains are the Corporate & Finance, \n",
    "Regulatory & IPR, HR, and IT departments, which provide the essential infrastructure to support domain diversity and business breadth. \n",
    "Together, these entities illustrate the intricate tapestry of operations within life sciences, where technology solutions are the connective \n",
    "tissue that binds these diverse areas into a cohesive, functioning whole.\"\"\"\n",
    "\n",
    "topic = \"Life Sciences\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial version of the prompt. There is a lot of room for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"You are a subject matter expert in {topic}. Your task is to compose an accurate and engaging narrative for this slide, intended to be read out to an audience during presentation.\n",
    "You will precise while referring to the contents of the slide.\n",
    "You will not use a first person voice.\n",
    "Do not use any salutations. \n",
    "Here is an example: {one_shot_example}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the text content using Multimodal Model and convert it to voice using the TTS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://entaipoc3.openai.azure.com//openai/deployments/gpt-4/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://entaipoc3.openai.azure.com//openai/deployments/gpt-4/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://entaipoc3.openai.azure.com//openai/deployments/gpt-4/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Function for processing a single file\n",
    "def generate_narrative(folder_path, slide_name, multi_modal_llm, speech_config, prompt):\n",
    "    image_file_path = f\"{folder_path}/{slide_name}.jpg\"\n",
    "    \n",
    "    # Assuming you have a way to load image data and generate text\n",
    "    image_documents = SimpleDirectoryReader(input_files=[image_file_path]).load_data()\n",
    "    response = multi_modal_llm.complete(\n",
    "        prompt=prompt,  # You need to define the prompt\n",
    "        image_documents=image_documents,\n",
    "    )\n",
    "    \n",
    "    # Saving the generated text to a file\n",
    "    text_file_path = f\"{folder_path}/{slide_name}.txt\"\n",
    "    with open(text_file_path, 'w') as file:\n",
    "        file.write(response.text)\n",
    "    \n",
    "    # Generating voice file from the text\n",
    "    voice_file_name = f\"{folder_path}/{slide_name}.mp3\"\n",
    "    file_config = speechsdk.audio.AudioOutputConfig(filename=voice_file_name)\n",
    "    speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=file_config)\n",
    "    speech_synthesizer.speak_text_async(response.text).get()\n",
    "\n",
    "def generate_narrative_for_all_images(folder_path, multi_modal_llm, speech_config, prompt):\n",
    "    for image_file_path in glob.glob(f\"{folder_path}/*.jpg\"):\n",
    "        file_name = os.path.basename(image_file_path).replace('.jpg', '')\n",
    "        generate_narrative(folder_path, file_name, multi_modal_llm, speech_config, prompt)\n",
    "\n",
    "generate_narrative_for_all_images(\n",
    "            folder_path=process_folder_path, \n",
    "            multi_modal_llm=multi_modal_llm, \n",
    "            speech_config=speech_config,\n",
    "            prompt=prompt\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error handling for text to speech conversion. Should be incorporated for production implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "#     print(\"Speech synthesized to speaker for text [{}]\".format(response.text))\n",
    "# elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "#     cancellation_details = result.cancellation_details\n",
    "#     print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
    "#     if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "#         if cancellation_details.error_details:\n",
    "#             print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "#     print(\"Did you update the subscription info?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdfread",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
