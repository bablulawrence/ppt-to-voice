{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import win32com.client\n",
    "\n",
    "# Combining imports from the same module/package\n",
    "from llama_index import (\n",
    "    VectorStoreIndex, ServiceContext,  set_global_service_context\n",
    ")\n",
    "from llama_index.llms import AzureOpenAI\n",
    "from llama_index.embeddings import AzureOpenAIEmbedding\n",
    "from llama_index.readers import SimpleDirectoryReader\n",
    "from llama_index.node_parser import UnstructuredElementNodeParser\n",
    "from llama_index.readers.file.flat_reader import FlatReader\n",
    "from llama_index.schema import IndexNode\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.agent import OpenAIAgent\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# Go up two levels to the root directory\n",
    "root_dir = os.path.dirname(os.path.dirname(current_dir))\n",
    "\n",
    "inputFilePath = os.path.join(root_dir, 'data', 'input', 'Life Sciences Regulatory 101.pptx')\n",
    "output_folder = os.path.join(root_dir, 'data', 'output')\n",
    "\n",
    "logging.basicConfig(\n",
    "    # stream=sys.stdout, level=logging.DEBUG\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    "\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import aspose.slides as slides\n",
    "# import aspose.pydrawing as drawing\n",
    "\n",
    "# with slides.Presentation(inputFilePath) as presentation:\n",
    "#     for slide in presentation.slides:\n",
    "#         slide.get_thumbnail(2, 2).save(\"presentation_slide_{0}.jpg\".format(str(slide.slide_number)), drawing.imaging.ImageFormat.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\61052067\\repos\\ppt-to-voice\\data\\output\\images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "powerpoint = win32com.client.Dispatch(\"PowerPoint.Application\")\n",
    "\n",
    "# Open the PowerPoint file\n",
    "# presentation_path = \"path_to_your_presentation.pptx\"  # Update this path\n",
    "presentation = powerpoint.Presentations.Open(inputFilePath)\n",
    "\n",
    "# Make sure the script waits until the presentation is fully loaded\n",
    "powerpoint.Visible = True\n",
    "\n",
    "# Path where you want to save the JPEG files\n",
    "image_folder = f\"{output_folder}\\images\" \n",
    "if not os.path.exists(image_folder):\n",
    "    os.makedirs(image_folder)\n",
    "\n",
    "print(image_folder)\n",
    "# Loop through each slide in the presentation and save as JPEG\n",
    "for i, slide in enumerate(presentation.Slides):\n",
    "    slide_name = f\"slide_{i+1}.jpg\"\n",
    "    slide.Export(os.path.join(image_folder, slide_name), \"JPG\")\n",
    "\n",
    "# Clean up the PowerPoint application\n",
    "presentation.Close()\n",
    "powerpoint.Quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_API_ENDPOINT\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "api_key_lmm = os.getenv(\"OPENAI_API_KEY_LMM\")\n",
    "azure_endpoint_lmm = os.getenv(\"AZURE_OPENAI_API_ENDPOINT_LMM\")\n",
    "api_version_lmm = os.getenv(\"AZURE_OPENAI_API_VERSION_LMM\")\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    engine=\"gpt-4\",\n",
    "    model=\"gpt-4\",\n",
    "    # engine=\"gpt-35-turbo-16k\",\n",
    "    # model=\"gpt-35-turbo-16k\",\n",
    "    temperature=0.0,\n",
    "    azure_endpoint= azure_endpoint,\n",
    "    api_key= api_key,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "multi_modal_llm = AzureOpenAIMultiModal(\n",
    "    engine=\"gpt-4\",\n",
    "    model=\"gpt-4-vision-preview\",\n",
    "    # engine=\"gpt-35-turbo-16k\",\n",
    "    # model=\"gpt-35-turbo-16k\",\n",
    "    temperature=0.0,\n",
    "    api_key= api_key_lmm,\n",
    "    azure_endpoint= azure_endpoint_lmm,\n",
    "    api_version=api_version_lmm,\n",
    "    max_tokens = 4096\n",
    ")\n",
    "\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"text-embedding-ada-002\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    "    chunk_size=1\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://entaipoc3.openai.azure.com//openai/deployments/gpt-4/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://entaipoc3.openai.azure.com//openai/deployments/gpt-4/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://entaipoc3.openai.azure.com//openai/deployments/gpt-4/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://entaipoc3.openai.azure.com//openai/deployments/gpt-4/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "The organization of Life Sciences Business Entities is structured to encompass both domain diversity and business breadth. On the domain diversity side, we have Research and Discovery, which includes target identification and drug development, as well as Preclinical Studies and Analytical/Process Development. The Manufacturing and Supply Chain Operations include GMP Manufacturing, Quality Control and Maintenance, and Warehouse/Logistics. On the business breadth side, we have Sales and Marketing Operations, which cover market access and commercial operations, as well as New Product Launches and Pricing and Reimbursement. Additionally, we have Corporate and Regulatory Management, which includes regulatory submissions and compliance, as well as IT and HR support. This structure allows for a comprehensive approach to the life sciences industry, ensuring that all aspects of the business are addressed and integrated.\n"
     ]
    }
   ],
   "source": [
    "image_path =  f\"{image_folder}/slide_5.jpg\"\n",
    "image_documents = SimpleDirectoryReader(input_files=[image_path]).load_data()\n",
    "\n",
    "response = multi_modal_llm.complete(\n",
    "    prompt=\"Please make a paragraph based on this slide that can be read out to the audience during the presentation. Do not include any salutations. Use 3rd person voice\",\n",
    "    image_documents=image_documents,\n",
    ")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "speech_key = os.getenv(\"AZURE_SPEECH_SERVICE_KEY\")\n",
    "service_region = os.getenv(\"AZURE_SPEECH_SERVICE_REGION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "speech_config.speech_synthesis_voice_name = \"en-US-AvaNeural\"\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = speech_synthesizer.speak_text_async(response.text).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_synthesis_to_mp3_file():\n",
    "    \"\"\"performs speech synthesis to a mp3 file\"\"\"\n",
    "    # Creates an instance of a speech config with specified subscription key and service region.\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "    # Sets the synthesis output format.\n",
    "    # The full list of supported format can be found here:\n",
    "    # https://docs.microsoft.com/azure/cognitive-services/speech-service/rest-text-to-speech#audio-outputs\n",
    "    speech_config.set_speech_synthesis_output_format(speechsdk.SpeechSynthesisOutputFormat.Audio16Khz32KBitRateMonoMp3)\n",
    "    # Creates a speech synthesizer using file as audio output.\n",
    "    # Replace with your own audio file name.\n",
    "    speech_config.speech_synthesis_voice_name = \"en-US-AvaNeural\"\n",
    "    file_name = f\"{output_folder}/outputaudio.mp3\"\n",
    "    file_config = speechsdk.audio.AudioOutputConfig(filename=file_name)\n",
    "    speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=file_config)\n",
    "\n",
    "    result = speech_synthesizer.speak_text_async(response).get\n",
    "    # Receives a text from console input and synthesizes it to mp3 file.\n",
    "        # Check result\n",
    "    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        print(\"Speech synthesized audio was saved to [{}]\".format(file_name))\n",
    "    elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = result.cancellation_details\n",
    "        print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "    print(\"Speech synthesized to speaker for text [{}]\".format(response.text))\n",
    "elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "    cancellation_details = result.cancellation_details\n",
    "    print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
    "    if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "        if cancellation_details.error_details:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "    print(\"Did you update the subscription info?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdfread",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
